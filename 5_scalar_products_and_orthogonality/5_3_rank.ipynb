{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying orthogonality to linear equations\n",
    "\n",
    "[Theorem 2.3](../5_2_orthogonal_bases_positive_definite_case.ipynb#Theorem-2.3) of the preceding section has an interesting application to the theory of linear equations. We consider such a system:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(**) && \\begin{array}{l}\n",
    "a_{1 1} x_1 + ... + a_{1 n} x_n &= 0 \\\\\n",
    "\\vdots \\\\\n",
    "a_{m 1} x_1 + ... + a_{m n} x_n &= 0\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can interpret its space of solutions in three ways:\n",
    "- (a) It consists of those vectors $X$ giving linear relations\n",
    "$$\n",
    "x_1 A^1 + .. x_n A^n = \\mathit{0}\n",
    "$$\n",
    "between the columns of A.\n",
    "- (b) The solutions form the space orthogonal to the row vectors of the matrix $A$.\n",
    "- (c)  The solutions form the kernel of the linear map represented by $A$, i.e. are the solutions of the equation $AX = \\mathit{0}$.\n",
    "\n",
    "The linear equations are assumed to have coefficients $a_{i j}$ in a field $K$. The analogue of theorem 2.3 is true for the scalar product on $K^n$. Indeed, let $W$ be a subspace of $K^n$ and let $W^\\perp$ be the subset of all elements $X \\in K^n$ such that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X \\cdot Y &= 0 & \\text{for all $Y \\in W$.}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then $W^\\perp$ is a subspace of $K^n$. Observe that we can have $X \\cdot X = 0$ even if $X \\neq \\mathit{0}$. For instance, let $K = C$ be the complex numbers and let $X =(1,i)$. Then $X \\cdot X = 1 - 1 = \\mathit{0}$. However, the analogue of theorem 2.3 is still true, namely:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 3.1\n",
    "\n",
    "Let $W$ be a subspace of $K^n$. Then\n",
    "\n",
    "$$\n",
    "\\dim W + \\dim W^\\perp = n\n",
    "$$\n",
    "\n",
    "We shall [prove this theorem in chapter 5.6, theorem 6.4](./5_6_dual_space_and_scalar_products.ipynb#Theorem-6.4:-The-dimension-of-a-vector-space-is-the-sum-of-the-dimension-of-a-subspace-and-the-dimension-of-its-orthogonal-complement). Here we shall apply it to the study of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Column rank and row rank\n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix such that $A = (a_{i j})$.\n",
    "\n",
    "Then its columns $A^1, ..., A^n$ generate a subspace, whose *dimension* is called the **column rank** of $A$.\n",
    "\n",
    "And its rows $A_1, ..., A_m$ generate a subspace, whose *dimension* is called the **row rank** of $A$.\n",
    "\n",
    "We may also say that the column rank of $A$ is the maximum number of linearly independent columns, and the row rank is the maximum number of linearly independent rows of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 3.2. \n",
    "\n",
    "Let $A = (a_{i j})$ be an $m \\times n$ matrix. Then the row rank and the column rank of $A$ are equal to the same number $r$. Furthermore, $n - r$ is the dimension of the space of solutions of the system of linear equations [(\\*\\*)](#Applying-orthogonality-to-linear-equations).\n",
    "\n",
    "#### Proof\n",
    "\n",
    "We first interpret (\\*\\*) from the columns perspective. Consider the map\n",
    "\n",
    "$$\n",
    "L: K^n \\to K^m\n",
    "$$\n",
    "\n",
    "defined as\n",
    "\n",
    "$$\n",
    "L(X) = x_1 A^1 + ... + x_n A^n\n",
    "$$\n",
    "\n",
    "where $X = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$.\n",
    "\n",
    "This map is obviously linear. Its image consists of the space generated by the column vectors of $A$. Its kernel is by definition the space of solutions of the system of linear equations.\n",
    "\n",
    "Then we have the column rank being the dimension of the image, and the space of solutions being the kernel. Then by [theorem 3.2 of Chapter 3.3](../3_linear_mappings/3_3_kernel_and_image_of_a_linear_map.ipynb#Theorem-3.2:-Kernel,-image,-and-dimensions),\n",
    "\n",
    "$$\n",
    "\\text{column rank} + \\dim \\text{space of solutions} = n\n",
    "$$\n",
    "\n",
    "For row vectors, we can interpret (\\*\\*) from the row perspective. Let the scalar product of $K^n$ be the dot product. Let $W$ be the space generated by $A_1, ..., A_m$. Then the row rank of $A$ is $\\dim W$, and the solution space of (\\*\\*) is the orthogonal complement is $W^\\perp$.\n",
    "\n",
    "Then by [theorem 2.6 in chapter 5.2](./5_2_orthogonal_bases_positive_definite_case.ipynb#Theorem-2.6), we have\n",
    "\n",
    "$$\n",
    "\\text{row rank} + \\dim \\text{space of solutions} = n\n",
    "$$\n",
    "\n",
    "Hence we have $\\text{column rank} = \\text{row rank}$. Q.E.D.\n",
    "\n",
    "In view of theorem 3.2, the row rank, or the column rank, is also called the rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: The rank of a matrix is the dimension of the image of the associated linear map\n",
    "\n",
    "Let $A$ be a $m \\times n$ matrix over $K$. Then the linear map associated to $A$, $L_A: K^n \\to K^m$ is defined as\n",
    "\n",
    "$$\n",
    "X \\mapsto A X\n",
    "$$\n",
    "\n",
    "Let $X = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$ Then $L_A$ is described by\n",
    "\n",
    "$$\n",
    "L_A(X) = x_1 A^1 + ... + x_n A^n\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\operatorname{rank} A = \\dim \\operatorname{Im} L_A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $b_1 , ..., b_m$ be numbers, and consider the system of inhomogeneous equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(*) && \\begin{array}{l}\n",
    "A_1 \\cdot X &= b_1 \\\\\n",
    "\\vdots \\\\\n",
    "A_m \\cdot X &= b_m\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "It may happen that this system has no solution at all, i.e. that the equations are inconsistent. For instance, the system\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "2 x + 3 y - z &=& 1 \\\\\n",
    "2 x + 3 y - 2 &=& 2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "has no solution. However, if there is at least one solution, then all solutions are obtainable from this one by adding an arbitrary solution of the associated homogeneous system (\\*\\*) (cf Exercise 7). Hence in this case again, we can speak of the **dimension** of the set of solutions. It is the **dimension** of the associated homogeneous system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Find the rank of the matrix\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & 1 \\\\\n",
    "0 & 1 & -1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Solution\n",
    "\n",
    "\n",
    "There are only two rows, so the rank is at most $2$. On the other hand, the two columns\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "0\n",
    "\\end{pmatrix} && \\text{and} &&\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "are linearly independent, for if $a, b$ are numbers such that\n",
    "\n",
    "$$\n",
    "a \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + b \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "2 a + b &=& 0 \\\\\n",
    "b &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So that $a = 0$. Therefore the two columns are linearly independent, and the rank is equal to $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.\n",
    "\n",
    "Find the dimension of the set of solutions of the following system of equations, and determine this set in $R^3$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "2 x + y + z &=& 1 \\\\\n",
    "y - z &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We know a solution is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{array}{rcl}\n",
    "x &=& \\frac{1}{2} \\\\\n",
    "y &=& 1 \\\\\n",
    "z &=& 1\n",
    "\\end{array}&& (*)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We first form the matrix of the system\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And we know the rank of this matrix is $2$. Hence the dimension of the set of the solution is $3 - 2 = 1$. Hence $(*)$ is the only solution to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3. Find a basis for the space of solutions of the equation\n",
    "\n",
    "$$\n",
    "3 x - 2 y + z = 0\n",
    "$$\n",
    "\n",
    "Let $A = \\begin{pmatrix} 3 \\\\ -2 \\\\ 1 \\end{pmatrix}$. Then the space of the solution to the equation above is the space orthogonal to $A$, which is also the space of vectors orthogonal to $\\operatorname{span} \\{A\\}$. Let $W = \\operatorname{span} \\{A\\}$.\n",
    "\n",
    "We know $\\dim W = 1$. By [theorem 3.1](#Theorem-3.1) we have $\\dim W^\\perp = \\dim R^3 - \\dim W = 2$.\n",
    "\n",
    "There are many base of $W^\\perp$. To find one, we extend $A$ to a basis of $R^3$. For instance, take $B = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $C = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n",
    "\n",
    "To show $A, B, C$ are linearly independent, let $a, b, c$ be numbers such that\n",
    "\n",
    "$$\n",
    "a A + b B + c C = \\mathit{0}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcr}\n",
    "3 a + 0 b + 0 c &=& 0 \\\\\n",
    "-2 a + 1 b + 0 c &=& 0 \\\\\n",
    "1 a + 0 b + 1 c &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcr}\n",
    "3 a &=& 0 \\\\\n",
    "-2 a + b &=& 0 \\\\\n",
    "a + c &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then we have $a = b = c = 0$. Hence $A, B, C$ are linearly independent, thus $\\{A, B, C\\}$ is a basis of $R^3$.\n",
    "\n",
    "And by [corollary 2.2 in chapter 5.2](../5_2_orthogonal_bases_positive_definite_case.ipynb#Corollary-2.2), we can find an orthogonal basis $\\{A^\\prime, B^\\prime, C^\\prime\\}$ of $R^3$ by applying Gram-Schmidt process on $\\{A, B, C\\}$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "A^\\prime &=& A \\\\\n",
    "B^\\prime &=& B - \\frac{\\langle B, A \\rangle}{\\langle A, A \\rangle} A &=& \\begin{pmatrix} \\frac{3}{7} \\\\ \\frac{5}{7} \\\\ \\frac{1}{7} \\end{pmatrix} \\\\\n",
    "C^\\prime &=& C - \\frac{\\langle C, A \\rangle}{\\langle A, A \\rangle} A - \\frac{\\langle C, B^\\prime \\rangle}{\\langle B^\\prime, B^\\prime \\rangle} B^\\prime &=& \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{14} \\begin{pmatrix} 3 \\\\ -2 \\\\ 1 \\end{pmatrix} - \\frac{1}{35} \\begin{pmatrix} 3 \\\\ 5 \\\\ 1 \\end{pmatrix}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Since $\\{A, B^\\prime, C^\\prime\\} = \\{A^\\prime, B^\\prime, C^\\prime\\}$ is an orthogonal basis of $R^3$, then $A$ is orthogonal to $B^\\prime, C^\\prime$. Hence $\\{B^\\prime, C^\\prime\\}$ is a basis of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
